<!DOCTYPE HTML>
<html>
    <head>
        <title>Shuran Song</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=1000">
        <link rel="stylesheet" href="https://use.typekit.net/quv7bsd.css"> <!-- fonts -->
        <link rel="stylesheet" href="style.css" />

        <script>
             (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
             (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
             m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
             })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
             ga('create', 'UA-89797207-1', 'auto');
             ga('send', 'pageview');
       </script>
    </head>
    <body id="body">
        <div id="main"> 
            <header id="header">
                <a href="index.html">HOME</a>&nbsp;&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;
                <a href="https://real.stanford.edu/">LAB</a>
            </header>
            <div id="profile">
                <div id="profile-pic">
                    <img src="images/people/shuran_song.jpg">
                    <p>
                        <a href="https://scholar.google.com/citations?hl=en&user=5031vK4AAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a>&nbsp;&nbsp;/&nbsp;&nbsp;<a href="https://twitter.com/SongShuran">Twitter</a>
                    </p>
                </div>
                <div id="profile-intro">
                    <div id="profile-name">Shuran Song</div>
                    <p>
                        Assistant Professor of Electrical Engineering, by courtesy, of Computer Science at <a href="http://stanford.edu/"> Stanford University</a> <br>
                        I lead the Robotics and Embodied AI Lab at Stanford University(<a href="https://real.stanford.edu/"> REAL@Stanford </a>).  We are interested in developing algorithms that enable intelligent systems to learn from their interactions with the physical world, and autonomously acquire the perception and manipulation skills necessary to execute complex tasks and assist people. 
                    
                        To learn more about my group's research please visit our <a href="https://real.stanford.edu/"> REAL website</a>.
                    <p>
                        Email: shuran [at] stanford [dot] edu <br> 
                        Office: RM258, 350 Jane Stanford Way Packard Bldg Stanford, CA 94305. 
                    </p>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="section recent-work">
                <div class="slider">
                    
                    <!-- <a href="https://umi-on-legs.github.io/"><video playsinline="" muted="" autoplay="" loop="">
                        <source src="images/projects/umi-on-leg.mp4" type="video/mp4">
                    </video>
                    </a> -->


                     <a href="https://umi-data.github.io/"><video playsinline="" muted="" autoplay="" loop="">
                        <source src="images/projects/umi-data.mp4" type="video/mp4">
                    </video>
                    </a>

                    <a href="https://umi-gripper.github.io/"><video playsinline="" muted="" autoplay="" loop="">
                        <source src="images/projects/umi_thumbnail.mp4" type="video/mp4">
                    </video>
                    </a>
                    <a href="http://diffusion-policy.cs.columbia.edu/"><img src="images/projects/diffusion.gif"></a>
                    <a href="http://dextairity.cs.columbia.edu/"><img src="images/projects/air.gif"></a>
                    <a href="http://irp.cs.columbia.edu/"><img src="images/projects/irp.jpeg"></a>
                    <a href="http://flingbot.cs.columbia.edu/"><img src="https://flingbot.cs.columbia.edu/images/teaser.png"></a>
                    <!-- <a href="https://tossingbot.cs.princeton.edu/"><img src="images/projects/tossingbot.png"></a> -->
                    <!-- <a href="http://dsr-net.cs.columbia.edu/"><img src="images/projects/dsr.gif"></a> -->
                    <!-- <a href="http://graspinwild.cs.columbia.edu/"><img src="images/projects/graspinwild.jpg"></a> -->
                    <!-- <a href="http://form2fit.github.io/"><img src="images/projects/form2fit.png"></a> -->
                   
                    <!-- <a href="https://sites.google.com/view/cleargrasp"><img src="images/projects/cleargrasp.gif"></a> -->
                    <!--  <a href="https://spatial-action-maps.cs.princeton.edu/"><img src="images/projects/sam.png"></a> -->
                </div>
            </div>

            <div class="divider"></div>
            
            <div class="section">
                <h1>Recent Talks</h1>
                <p>
                    <ul>

                        <li> Cross-Embodiment Learning for Dexterous Manipulation  
                        (<a href="https://www.youtube.com/watch?v=cUKNbwqElCw"> Recording </a>) at <a href="https://sites.google.com/view/dexterity-workshop-icra2025/home"> Handy Moves Workshop</a> @ ICRA'25 </li>


                        <li> Self-Adaptive Robots
                        (<a href="https://www.dropbox.com/scl/fi/qkfka04zepcj4vydnhcd3/RSS_EARL.pdf?rlkey=ukg014p9rkz9fgh25theyozjk&dl=0"> Slides </a>) at <a href="https://earl.robot-learning.net/"> Workshop on Embodiment-Aware Robot Learning (EARL) </a> @ RSS'24 </li>


                        <li> Navigating the Upward Spiral for Sensorimotor Learning
                        (<a href="https://www.dropbox.com/scl/fi/6j07bw2pxg96gqrmq008z/ICRA24_futureroadmap.pdf?rlkey=pv7ht9g1d8deakbdht5j7u6y0&dl=0"> Slides </a>) Keynote at <a href="https://icra-manipulation-skill.github.io/"> A Future Roadmap for Sensorimotor Skill Learning for Robot Manipulation </a> @ ICRAâ€™24 </li>


                        <li> What I wish I had for Robot Learning 
                        (<a href="https://www.dropbox.com/scl/fi/obcik2tpg7py3i86yczrv/CoRL23_conference.pdf?rlkey=bunk5ptmwjbdqclwe47jnkl8b&dl=0"> Slides </a>)   Early Career Keynote at <a href="https://www.corl2023.org/speakers">Conference on Robot Learning </a> </li>

                        <li> Learning Meets Gravity: Robots that Embrace Dynamics and Pixels
                        (<a href="https://www.ri.cmu.edu/event/learning-meets-gravity-robots-that-learn-to-embrace-dynamics-from-data/"> Talk </a>)  at CMU Robotics Institute</li>

                        <span id="moreTalks">
                        <li> Abstraction as Inductive Bias: Open-World 3D Scene Understanding without Open-World 3D Data 
                        (<a href="https://www.cs.columbia.edu/~shurans/talks/CoRL_inductive_bias.pdf"> Slide </a>)  at <a href="https://sites.google.com/view/corl-2022-inductive-bias-ws/home?authuser=0">  Inductive Bias in Robot Learning Workshop</a> @ CoRL'22</li>

                        <li> The Reasonable Effectiveness of Dynamic Manipulation for Deformable Objects(<a href="https://www.youtube.com/watch?v=Pia-V67ishw&ab_channel=UniversityofTorontoRoboticsInstitute"> Talk </a>)  at University of Toronto Robotics Institute  </li>

                        
                        <li> Structure from Action: Articulated Object Structure Discovery with Active Interactions (<a href="https://www.cs.columbia.edu/~shurans/talks/StrcturefromAction.pdf"> Slides </a>)  at ICCV <a href="https://geometry.stanford.edu/struco3d/"> Struco3D Workshop </a>  </li>
                        <li> Self-Adaptive Manipulation  (<a href="https://www.cs.columbia.edu/~shurans/talks/self-adaptive-grasping.pdf"> Slides </a>)  (<a href="https://www.youtube.com/watch?v=f2OZPwhSQu4&ab_channel=AllenInstituteforAI"> Talk </a>)  </li>
                    	<li>Unfolding the Unseen: Deformable Cloth Perception and Manipulation  (<a href="https://www.cs.columbia.edu/~shurans/talks/unfold_unsceen.m4v"> Slides Video </a> <a href="https://www.cs.columbia.edu/~shurans/talks/Cloth_Precpetion.pdf"> PDF</a>)  (<a href="https://www.youtube.com/watch?v=Ek20Zw77QPU&ab_channel=3DGVSeminar"> Talk </a>)  </li>
                        <li>Active Scene Understanding with Robot Interactions (<a href="https://www.cs.columbia.edu/~shurans/talks/2020_active_scene.pdf"> Slides </a>)  </li>
                        
                        
                        <li>Category-level Pose Estimation (<a href="https://www.cs.columbia.edu/~shurans/talks/Pose_Estimation_ECCV_2020.pdf"> Slides </a>) at ECCV2020  <a href="http://cmp.felk.cvut.cz/sixd/workshop_2020/">  Workshop on Recovering 6D Object Pose </a></li>
                        <li>Learning Visual Representations for Generalizable Manipulation  (<a href="https://www.cs.columbia.edu/~shurans/talks/visualrep_manuplation_cvpr2020.pdf"> Slides </a>) at CVPR2020 <a href="https://scene-understanding.com/">  3D Scene Understanding for Vision, Graphics, and Robotics </a></li>
                        </span>

                        <div onclick="toggleTalks()" id="moreTalksBtn" class="showBtn"><a>Show more...</a></div>
                        <div onclick="toggleTalks()" id="lessTalksBtn" class="showBtn"><a>Show less...</a></div>
                        
                    </ul>
                </p>
                <div style="clear: both;"></div>
            </div>

            <div class="divider"></div>
            <div class="section research">
                <h1>Publications</h1>


                <div class="research-proj">
                    <a href="https://dex-umi.github.io/" class="research-thumb">
                    
                    <video playsinline="" muted="" autoplay="" loop="" width="180px">
                        <source src="images/projects/dexumi_thumbnail.mp4" type="video/mp4">
                    </video>
                    </a>

                    
                    <a href="https://dex-umi.github.io/" class="research-proj-title">  DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation </a>
                    <p> Mengda Xu*, Han Zhang*, Yifan Hou, Zhenjia Xu, Linxi Fan, Manuela Veloso, Shuran Song <br>
                        Preprint, 2025 <br>
                       <a href="https://dex-umi.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/pdf/2505.21864v2">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/real-stanford/DexUMI">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://dex-umi.github.io/tutorial/hardware.html">Hardware</a> 
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://unified-video-action-model.github.io/" class="research-thumb">
                    <img src="images/projects/UVA.png" alt="" />
                    </a>


                    <a href="https://unified-video-action-model.github.io/" class="research-proj-title">  Unified Video Action Model </a>
                    <p> Shuang Li, Yihuai Gao, Dorsa Sadigh, Shuran Song <br>
                        RSS 2025 <br>
                       <a href="https://unified-video-action-model.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2503.00200">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://unified-video-action-model.github.io/">Code+Model</a> 
                    </p>
                </div>
                <br>
                <div class="research-proj">
                    <a href="https://robopanoptes.github.io//" class="research-thumb">
                    
                    <video playsinline="" muted="" autoplay="" loop="" width="180px">
                        <source src="images/projects/robopanotes.mp4" type="video/mp4">
                    </video>
                    </a>


                    <a href="https://robopanoptes.github.io/" class="research-proj-title"> RoboPanoptes: The All-Seeing Robot with Whole-body Dexterity </a>
                    <p> Xiaomeng Xu,  Dominik Bauer,  Shuran Song  <br>
                        RSS 2025 <br>
                       <a href="https://robopanoptes.github.io/"> Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2501.05420"> Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://robopanoptes.github.io/"> Hardware+Code </a> 
                    </p>
                </div>
                <br>
                
                <div class="research-proj">
                    <a href="https://toddlerbot.github.io/" class="research-thumb">
                    <img src="images/projects/Toddy.png" alt="" />
                    </a>


                    <a href="https://toddlerbot.github.io/" class="research-proj-title">  ToddlerBot: Open-Source ML-Compatible Humanoid Platform for Loco-Manipulation </a>
                    <p> Haochen Shi, Weizhuo Wang, Shuran Song, C. Karen Liu <br>
                        arXiv 2025 <br>
                       <a href="https://toddlerbot.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2502.00893">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://toddlerbot.github.io/">Hardware+Code </a> 
                    </p>
                </div>

                

                
                <div class="research-proj">
                    <a href="https://aquabot.cs.columbia.edu/" class="research-thumb">
                    
                    <video playsinline="" muted="" autoplay="" loop="" width="180px">
                        <source src="images/projects/aquabot.mp4" type="video/mp4">
                    </video>
                    </a>


                    <a href="https://aquabot.cs.columbia.edu/" class="research-proj-title">  Self-Improving Autonomous Underwater Manipulation </a>
                    <p> Ruoshi Liu, Huy Ha, Mengxue Hou, Shuran Song, Carl Vondrick <br>
                        ICRA 2025 <br>
                       <a href="https://aquabot.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2410.18969">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/cvlab-columbia/aquabot">Code </a> 
                    </p>
                </div>


                <div class="research-proj">
                    <a href="https://adaptive-compliance.github.io/" class="research-thumb">
                    <img src="images/projects/acp.gif" alt="" />
                    </a>


                    <a href="https://adaptive-compliance.github.io/" class="research-proj-title">  Adaptive Compliance Policy: Learning Approximate Compliance for Diffusion Guided Control </a>
                    <p> Yifan Hou, Zeyi Liu, Cheng Chi, Eric Cousineau, Naveen Kuppuswamy, Siyuan Feng, Benjamin Burchfiel, and Shuran Song <br>
                        ICRA 2025 <br>
                       <a href="https://adaptive-compliance.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2410.09309">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/yifan-hou/adaptive_compliance_policy">Code </a> 
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://get-zero-paper.github.io/" class="research-thumb">
                    <img src="images/projects/get-zero.png" alt="" />
                    </a>


                    <a href="https://get-zero-paper.github.io/" class="research-proj-title">  GET-Zero: Graph Embodiment Transformer for Zero-shot Embodiment Generalization </a>
                    <p> Austin Patel and Shuran Song <br>
                        ICRA 2025 <br>
                       <a href="https://get-zero-paper.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/pdf/2407.15002">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/real-stanford/get_zero">Code </a> 
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://real2code.github.io/" class="research-thumb">
                        <img src="images/projects/real2code.png" alt="" />
                    </a>


                    <a href="https://real2code.github.io/" class="research-proj-title">Real2Code: Reconstruct Articulated Objects via Code Generation</a>
                    <p> Zhao Mandi, Yijia Weng, Dominik Bauer, Shuran Song <br>
                        ICRL 2025 <br>
                       <a href="https://real2code.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2406.08474">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/MandiZhao/real2code">Code </a> 
                    </p>
                </div>

                

                <div class="research-proj">
                    <a href="https://tidybot2.github.io/" class="research-thumb">
                    <img src="images/projects/wu2024tidybot.gif" alt="" />
                    </a>


                    <a href="https://tidybot2.github.io/" class="research-proj-title">   TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot Learning </a>
                    <p> Jimmy Wu, William Chong, Robert Holmberg, Aaditya Prasad, Yihuai Gao, Oussama Khatib, Shuran Song, Szymon Rusinkiewicz, Jeannette Bohg <br>
                        Conference on Robot Learning (CoRL 2024) <br>
                       <a href="https://tidybot2.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://tidybot2.github.io/">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/jimmyyhwu/tidybot2">Code </a> 
                       <a href="https://tidybot2.github.io/docs/">Hardware Guide </a> 
                    </p>
                </div>
                <div class="research-proj">
                    <a href="https://drrobot.cs.columbia.edu/" class="research-thumb">
                    <img src="images/projects/drrobot.png" alt="" />
                    </a>


                    <a href="https://drrobot.cs.columbia.edu/" class="research-proj-title">  Differentiable Robot Rendering</a>
                    <p> Ruoshi Liu, Alper Canberk, Shuran Song, Carl Vondrick<br>
                        Conference on Robot Learning (CoRL 2024) <br>
                       <a href="https://drrobot.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://drrobot.cs.columbia.edu/">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://drrobot.cs.columbia.edu/">Code </a> 
                    </p>
                </div>
                
                <div class="research-proj">
                    <a href="https://im-flow-act.github.io/" class="research-thumb">
                    <img src="images/projects/im2flow2act.png" alt="" />
                    </a>


                    <a href="https://im-flow-act.github.io/" class="research-proj-title">  Flow as the Cross-domain Manipulation Interface</a>
                    <p> Mengda Xu, Zhenjia Xu, Yinghao Xu, Cheng Chi, Gordon Wetzstein, Manuela Veloso, Shuran Song <br>
                        Conference on Robot Learning (CoRL 2024) <br>
                       <a href="https://im-flow-act.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://im-flow-act.github.io/static/pdf/Flow_as_the_Cross_Domain_Manipulation_Interface.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://im-flow-act.github.io/">Code </a> 
                    </p>
                </div>


                



                <div class="research-proj">
                    <a href="https://umi-on-legs.github.io/" class="research-thumb">
                    <video playsinline="" muted="" autoplay="" loop="" width="180px">
                        <source src="images/projects/umi-on-leg.mp4" type="video/mp4">
                    </video>
                    </a>



                    <a href="https://umi-on-legs.github.io/" class="research-proj-title">  UMI on Legs: Making Manipulation Policies Mobile with Manipulation-Centric Whole-body Controllers </a>
                    <p> Huy Ha*, Yihuai Gao*, Zipeng Fu, Jie Tan, Shuran Song <br>
                        Conference on Robot Learning (CoRL 2024) <br>
                       <a href="https://umi-on-legs.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://umi-on-legs.github.io/">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://umi-on-legs.github.io/">Code </a> 
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://mani-wav.github.io/" class="research-thumb">
                    <img src="images/projects/maniwav.png" alt="" />
                    </a>


                    <a href="https://mani-wav.github.io/" class="research-proj-title">  ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data </a>
                    <p> Zeyi Liu, Cheng Chi, Eric Cousineau, Naveen Kuppuswamy, Benjamin Burchfiel, Shuran Song <br>
                        Conference on Robot Learning (CoRL 2024) <br>
                       <a href="https://mani-wav.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2406.19464">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://mani-wav.github.io/">Code </a> 
                    </p>
                </div>


                


                <div class="research-proj">
                    <a href="https://equi-bot.github.io/" class="research-thumb">
                    <img src="images/projects/EquiBot.png" alt="" />
                    </a>


                    <a href="https://equi-bot.github.io/" class="research-proj-title"> EquiBot: SIM(3)-Equivariant Diffusion Policy for Generalizable and Data Efficient Learning </a>
                    <p> Jingyun Yang*, Zi-ang Cao*, Congyue Deng, Rika Antonova, Shuran Song, Jeannette Bohg <br>
                        Conference on Robot Learning (CoRL 2024) <br>
                       <a href="https://equi-bot.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2407.01479">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/yjy0625/equibot">Code </a> 
                    </p>
                </div>


                <div class="research-proj">
                    <a href="https://mani-wav.github.io/" class="research-thumb">
                    <img src="images/projects/dreamitate.gif" alt="" />
                    </a>


                    <a href="https://dreamitate.cs.columbia.edu/" class="research-proj-title"> Dreamitate: Real-World Visuomotor Policy Learning via Video Generation </a>
                    <p> Junbang Liang*, Ruoshi Liu*, Ege Ozguroglu, Sruthi Sudhakar, Achal Dave, Pavel Tokmakov, Shuran Song, Carl Vondrick <br>
                        Conference on Robot Learning (CoRL 2024) <br>
                       <a href="https://dreamitate.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2406.16862">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://dreamitate.cs.columbia.edu/">Code </a> 
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://dgdm-robot.github.io/" class="research-thumb">
                    <img src="images/projects/dgdm.jpeg" alt="" />
                    </a>


                    <a href="https://dgdm-robot.github.io/" class="research-proj-title">  Dynamics-Guided Diffusion Model for Robot Manipulator Design </a>
                    <p> Xiaomeng Xu, Huy Ha, Shuran Song <br>
                        Conference on Robot Learning (CoRL 2024) <br>
                       <a href="https://dgdm-robot.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2402.15038">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://dgdm-robot.github.io/">Code </a> 
                    </p>
                </div>


                <div class="research-proj">
                    <a href="https://paperbot.cs.columbia.edu/" class="research-thumb">
                    <video playsinline="" muted="" autoplay="" loop="" width="180px">
                        <source src="images/projects/paperbot.mp4" type="video/mp4">
                    </video>
                    </a>


                    <a href="https://paperbot.cs.columbia.edu/" class="research-proj-title">  PaperBot: Learning to Design Real-World Tools Using Paper </a>
                    <p> Ruoshi Liu, Junbang Liang, Sruthi Sudhakar, Huy Ha, Cheng Chi, Shuran Song, Carl Vondrick <br>
                        arXiv 2024 <br>
                       <a href="https://paperbot.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2403.09566">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/cvlab-columbia/paperbot">Code </a> 
                    </p>
                </div>

                



                <div class="research-proj">
                    <a href="https://paperbot.cs.columbia.edu/" class="research-thumb">
                    <video playsinline="" muted="" autoplay="" loop="" width="180px">
                        <source src="images/projects/doughnet.webm" type="video/mp4">
                    </video>
                    </a>


                    <a href="https://dough-net.github.io/" class="research-proj-title">  DoughNet: A Visual Predictive Model for Topological Manipulation of Deformable Objects </a>
                    <p> Dominik Bauer, Zhenjia Xu, Shuran Song <br>
                        ECCV 2024 <br>
                       <a href="https://dough-net.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2404.12524">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://dough-net.github.io/">Code </a> 

                    </p>
                </div>

                

                <div class="research-proj">
                    <a href="https://umi-gripper.github.io/" class="research-thumb">
                    <video playsinline="" muted="" autoplay="" loop="" width="180px">
                        <source src="images/projects/umi_thumbnail.mp4" type="video/mp4">
                    </video>
                    </a>


                    <a href="https://umi-gripper.github.io/" class="research-proj-title">  Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots </a>
                    <p> Cheng Chi*, Zhenjia Xu*, Chuer Pan, Eric Cousineau, Ben Burchfiel, Siyuan Feng, Russ Tedrake, Shuran Song <br>
                       RSS 2024 <br>
                       <strong>Outstanding System Paper Finalist</strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://umi-gripper.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2402.10329">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://umi-gripper.github.io/">Code </a> 
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://droid-dataset.github.io/" class="research-thumb">
                    <img src="images/projects/Dorid.png" alt="" />
                    </a>


                    <a href="https://droid-dataset.github.io/" class="research-proj-title">  DROID: A Large-Scale In-the-Wild Robot Manipulation Dataset </a>
                    <p> Alexander Khazatsky, Karl Pertsch et al <br>
                       RSS 2024 <br>
                       <a href="https://droid-dataset.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2403.12945">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://colab.research.google.com/drive/1b4PPH4XGht4Jve2xPKMCh-AXXAQziNQa?usp=sharing">Code </a> 
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://project-roco.github.io/" class="research-thumb">
                    <img src="images/projects/roco.jpg" alt="" />
                    </a>


                    <a href="https://project-roco.github.io/" class="research-proj-title">  RoCo: Dialectic Multi-Robot Collaboration with Large Language Models </a>
                    <p> Zhao Mandi, Shreeya Jain, Shuran Song <br>
                       International Conference on Robotics and Automation  (<b>ICRA 2024</b>)<br>
                       <a href="https://project-roco.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2307.04738">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://project-roco.github.io/">Code </a> 
                    </p>
                </div>


                <div class="research-proj">
                    <a href="https://robotics-transformer-x.github.io/" class="research-thumb">
                    <img src="images/projects/RT-x.png" alt="" />
                    </a>


                    <a href="https://robotics-transformer-x.github.io/" class="research-proj-title">  Open X-Embodiment: Robotic Learning Datasets and RT-X Models</a>
                    <p> Abhishek Padalkar et al <br>
                       International Conference on Robotics and Automation (ICRA 2024) <br>
                       <strong>Best Paper Award</strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://robotics-transformer-x.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/pdf/2310.08864.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://robotics-transformer-x.github.io/">Code </a> 
                    </p>
                </div>


                <div class="research-proj">
                    <a href="https://datacomp.ai/" class="research-thumb">
                    <img src="images/projects/datacomp.png" alt="" />
                    </a>

                    <a href="https://arxiv.org/abs/2304.14108" class="research-proj-title">  DataComp: In search of the next generation of multimodal datasets </a>
                    <p> Samir Yitzhak Gadre*, Gabriel Ilharco*, Alex Fang* et al. <br>
                       NeurIPS, 2023 (oral) <br>
                       <a href="https://datacomp.ai/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2304.14108">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/mlfoundations/datacomp">Code </a> 
                    </p>
                    <p> <br> </p>
                </div>

                
                <div class="research-proj">
                    <a href="https://www.cs.columbia.edu/~huy/scalingup/" class="research-thumb">

                    <video playsinline="" muted="" autoplay="" loop="" width="180px">
                        <source src="images/projects/scalingup-teaser.mp4" type="video/mp4">
                    </video>
                    </a>

                    <a href="https://www.cs.columbia.edu/~huy/scalingup/" class="research-proj-title">  Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition </a>
                    <p> Huy Ha, Pete Florence, Shuran Song <br>
                       Conference on Robot Learning 2023 <br>
                       <a href="https://www.cs.columbia.edu/~huy/scalingup/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2307.14535">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/columbia-ai-robotics/scalingup">Code </a> 
                    </p>
                </div>


                <div class="research-proj">
                    <a href="https://robot-reflect.github.io/" class="research-thumb">
                    
                    <img src="images/projects/reflect.png" alt="" />
                    <a href="https://robot-reflect.github.io/" class="research-proj-title">  REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction </a>
                    <p>
                       Zeyi Liu*, Arpit Bahety*, Shuran Song 
                       <br>
                       Conference on Robot Learning 2023 <br>
                       <a href="https://robot-reflect.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2306.15724">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/columbia-ai-robotics/reflect">Code </a> 
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://xskillcorl.github.io/" class="research-thumb">
                    <img src="images/projects/Xskill.png" alt="" />
                    <a href="https://xskillcorl.github.io/" class="research-proj-title">  XSkill: Cross Embodiment Skill Discovery </a>
                    <p>
                        Mengda Xu, Zhenjia Xu, Cheng Chi, Manuela Veloso, Shuran Song
                       <br>
                       Conference on Robot Learning 2023 <br>
                       <a href="https://xskillcorl.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2307.09955">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://xskillcorl.github.io/">Code </a> 
                    </p>
                </div>


                <div class="research-proj">
                    <a href="https://general-part-assembly.github.io/" class="research-thumb">
                    <img src="images/projects/GPAT.jpeg" alt="" />
                    <a href="https://general-part-assembly.github.io/" class="research-proj-title">  Rearrangement Planning for General Part Assembly </a>
                    <p>
                       Yulong Li, Andy Zeng, Shuran Song
                       <br>
                       Conference on Robot Learning 2023 <br>
                       <strong>Oral Presentation</strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://general-part-assembly.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/pdf/2307.00206.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://general-part-assembly.github.io/">Code </a> 
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://tidybot.cs.princeton.edu/" class="research-thumb"><img src="images/projects/wu2023tidybot.gif" alt="" /></a>
                    <a href="https://tidybot.cs.princeton.edu/" class="research-proj-title">  TidyBot: Personalized Robot Assistance with Large Language Models </a>
                    <p>
                       Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, Thomas Funkhouser<br>
                       Autonomous Robots (AuRo) - Special Issue: Large Language Models in Robotics, 2023 <br>
                       IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2023 <br>
                       <a href="https://tidybot.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2305.05658">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/jimmyyhwu/tidybot">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       
                       
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://sfa.cs.columbia.edu/" class="research-thumb"><img src="images/projects/sfa.png" alt="" /></a>
                    <a href="https://sfa.cs.columbia.edu/" class="research-proj-title"> Structure From Action: Learning Interactions for Articulated Object 3D Structure Discovery </a>
                    <p>
                       Neil Nie, Samir Yitzhak Gadre, Kiana Ehsani, Shuran Song <br>
                       IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2023 <br>
                       <a href="https://sfa.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2207.08997">Paper</a>
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://bag-all-you-need.cs.columbia.edu/" class="research-thumb"><img src="images/projects/bag.png" alt="" /></a>
                    <a href="https://bag-all-you-need.cs.columbia.edu/" class="research-proj-title"> Bag All You Need: Learning a Generalizable Bagging Strategy for Heterogeneous Objects </a>
                    <p>
                       Arpit Bahety*, Shreeya Jain*, Huy Ha, Nathalie Hager, Benjamin Burchfiel, Eric Cousineau, Siyuan Feng, Shuran Song <br>
                       IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2023 <br>
                       <a href="https://bag-all-you-need.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2210.09997">Paper</a>
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://diffusion-policy.cs.columbia.edu/" class="research-thumb"><img src="images/projects/diffusion.gif" alt="" /></a>
                    <a href="https://diffusion-policy.cs.columbia.edu/" class="research-proj-title"> Diffusion Policy: Visuomotor Policy Learning via Action Diffusion </a>
                    <p>
                       Cheng Chi,  Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, Shuran Song<br>
                       Robotics: Science and Systems (RSS)  2023<br>
                       <a href="https://diffusion-policy.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://diffusion-policy.cs.columbia.edu/">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp; 
                       <a href="https://github.com/columbia-ai-robotics/diffusion_policy">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp; 
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://roboninja.cs.columbia.edu/" class="research-thumb"><img src="images/projects/RoboNinja_RSS.gif" alt="" /></a>
                    <a href="https://roboninja.cs.columbia.edu/" class="research-proj-title"> RoboNinja: Learning an Adaptive Cutting Policy for Multi-Material Objects </a>
                    <p>
                       Zhenjia Xu, Zhou Xian, Xingyu Lin, Cheng Chi, Zhiao Huang, Chuang Gan, Shuran Song<br>
                       Robotics: Science and Systems (RSS)  2023 <br>
                       <a href="https://roboninja.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2302.11553">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp; 
                       <a href="https://github.com/columbia-ai-robotics/roboninja">Code & Simulation </a>&nbsp;&nbsp;&bull;&nbsp;&nbsp; 
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://cow.cs.columbia.edu/" class="research-thumb"><img src="images/projects/cow.png" alt="" /></a>
                    <a href="https://cow.cs.columbia.edu/" class="research-proj-title">CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation <br> (a.k.a Clip on Wheels) </a>
                    <p>
                       Samir Yitzhak Gadre, Mitchell Wortsman, Gabriel Ilharco, Ludwig Schmidt, Shuran Song <br>
                       Conference on Computer Vision and Pattern Recognition (CVPR 2022) <br>
                       <a href="https://cow.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2203.10421">Paper</a>
<!--                        &nbsp;&nbsp;&bull;&nbsp;&nbsp; <a href="">Code Coming Soon</a> -->
                       
                    </p>
                </div>



                <div class="research-proj">
                    <a href="https://clothfunnels.cs.columbia.edu/" class="research-thumb"><img src="images/projects/funnel.png" alt="" /></a>
                    <a href="https://clothfunnels.cs.columbia.edu/" class="research-proj-title"> Cloth Funnels: Canonicalized-Alignment for Multi-Purpose Garment Manipulation </a>
                    <p>
                       Alper Canberk, Cheng Chi, Huy Ha, Benjamin Burchfiel, Eric Cousineau, Siyuan Feng, Shuran Song<br>
                       International Conference on Robotics and Automation  (<b>ICRA 2023</b>)<br>
                       <a href="https://clothfunnels.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2210.09347">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp; 
                    </p>
                </div>




                <div class="research-proj">
                    <a href="https://jxu.ai/tandem3d/" class="research-thumb"><img src="images/projects/touch3d.png" alt="" /></a>
                    <a href="https://jxu.ai/tandem3d/" class="research-proj-title"> TANDEM3D: Active Tactile Exploration for 3D Object Recognition </a>
                    <p>
                       Jingxi Xu*, Han Lin*, Shuran Song, Matei Ciocarlie <br>
                       International Conference on Robotics and Automation  (<b>ICRA 2023</b>)<br>
                       <a href="https://jxu.ai/tandem3d/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2209.08772">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp; 
                    </p>
                </div>


                <div class="research-proj">
                    <a href="https://semantic-abstraction.cs.columbia.edu/" class="research-thumb"><img src="images/projects/semabs-rect.gif" alt="" /></a>
                    <a href="https://semantic-abstraction.cs.columbia.edu/" class="research-proj-title"> Semantic Abstraction: Open-World 3D Scene Understanding from 2D Vision-Language Models </a>
                    <p>
                       Huy Ha, Shuran Song <br>
                       Conference on Robot Learning (<b>CoRL2022</b>)<br>
                       <a href="https://semantic-abstraction.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/pdf/2207.11514.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp; 
                       <a href="https://github.com/columbia-ai-robotics/semantic-abstraction">Code</a> &nbsp;&nbsp;&bull;&nbsp;&nbsp; 
                       <a href="https://huggingface.co/spaces/huy-ha/semabs-relevancy">Demo on Huggingface</a>
                       
                    </p>
                </div>

                

                <div class="research-proj">
                    <a href="https://busybot.cs.columbia.edu/" class="research-thumb"><img src="images/projects/busybot.gif" alt="" /></a>
                    <a href="https://busybot.cs.columbia.edu/" class="research-proj-title"> BusyBot: Learning to Interact, Reason, and Plan in a BusyBoard Environment </a>
                    <p>
                       Zeyi Liu, Zhenjia Xu, Shuran Song <br>
                       Conference on Robot Learning (<b>CoRL2022</b>)<br>
                       <a href="https://busybot.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2207.08192">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp; 
                       <a href="https://github.com/columbia-ai-robotics/BusyBot">Code</a>  
                       
                       
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://aspire.cs.columbia.edu/" class="research-thumb"><img src="images/projects/aspire.png" alt="" /></a>
                    <a href="https://aspire.cs.columbia.edu/" class="research-proj-title"> ASPiRe: Adaptive Skill Priors for Reinforcement Learning </a>
                    <p>
                        Mengda Xu, Manuela Veloso, Shuran Song <br>
                        Conference on Neural Information Processing Systems (NeurIPS 2022) <br>
                       <a href="https://aspire.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://aspire.cs.columbia.edu/">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp; 
                       <a href="https://github.com/columbia-ai-robotics/ASPiRe">Code</a>  
                       
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://model-patching.github.io/" class="research-thumb"><img src="images/projects/paint.jpeg" alt="" /></a>
                    <a href="https://model-patching.github.io/" class="research-proj-title"> Patching open-vocabulary models by interpolating weights </a>
                    <p>
                        Gabriel Ilharco*, Mitchell Wortsman*, Samir Yitzhak Gadre*, Shuran Song, Hannaneh Hajishirzi Simon Kornblith, Ali Farhadi, Ludwig Schmidt <br>
                        Conference on Neural Information Processing Systems (NeurIPS 2022) <br>
                       <a href="https://model-patching.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2208.05592">Paper</a>
                       &nbsp;&nbsp;&bull;&nbsp;&nbsp; <a href="https://github.com/mlfoundations/patching">Code</a>
                       
                    </p>
                </div>

                <!-- <div class="research-proj">
                    <a href="https://cow.cs.columbia.edu/" class="research-thumb"><img src="images/projects/cow.png" alt="" /></a>
                    <a href="https://cow.cs.columbia.edu/" class="research-proj-title">CLIP on Wheels: Open-Vocabulary Models are (Almost) Zero-Shot Object Navigators</a>
                    <p>
                       Samir Yitzhak Gadre, Mitchell Wortsman, Gabriel Ilharco, Ludwig Schmidt, Shuran Song <br>
                       Arxiv Preprint <br>
                       <a href="https://cow.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2203.10421">Paper</a>
                       &nbsp;&nbsp;&bull;&nbsp;&nbsp; <a href="">Code Coming Soon</a>
                       
                    </p>
                </div> -->

                
                


                <div class="research-proj">
                    <a href="https://irp.cs.columbia.edu/" class="research-thumb"><img src="images/projects/irp.jpeg" alt="" /></a>
                    <a href="https://irp.cs.columbia.edu/" class="research-proj-title">Iterative Residual Policy for Goal-Conditioned Dynamic Manipulation of Deformable Objects</a>
                    <p>
                       Cheng Chi, Benjamin Burchfiel, Eric Cousineau, Siyuan Feng, Shuran Song <br>
                       Robotics: Science and Systems (RSS)  2022 <br>
                       <strong>Best Paper Award</strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp;
                      <strong>Best Student Paper Finalist</strong>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://irp.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2203.00663">Paper</a>
                       &nbsp;&nbsp;&bull;&nbsp;&nbsp; <a href="https://github.com/columbia-ai-robotics/irp">Code</a>
                       
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://dextairity.cs.columbia.edu/" class="research-thumb"><img src="images/projects/air.gif" alt="" /></a>
                    <a href="https://dextairity.cs.columbia.edu/" class="research-proj-title">DextAIRity: Deformable Manipulation Can be a Breeze</a>
                    <p>
                       Zhenjia Xu, Cheng Chi, Benjamin Burchfiel, Eric Cousineau, Siyuan Feng, Shuran Song <br>
                       Robotics: Science and Systems (RSS)  2022 <br>
                       <strong>Best System Paper Finalist</strong>  &nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://dextairity.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2203.01197">Paper</a>
                       &nbsp;&nbsp;&bull;&nbsp;&nbsp; <a href="https://github.com/columbia-ai-robotics/dextairity">Code</a>
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://learning-dynamic-manipulation.cs.princeton.edu/" class="research-thumb"><img src="images/projects/air-wu.gif" alt="" /></a>
                    <a href="https://learning-dynamic-manipulation.cs.princeton.edu/" class="research-proj-title">Learning Pneumatic Non-Prehensile Manipulation with a Mobile Blower</a>
                    <p>
                       Jimmy Wu, Xingyuan Sun, Andy Zeng, Shuran Song, Szymon Rusinkiewicz, Thomas Funkhouser <br>
                       Robotics and Automation Letters (RA-L) 2022</i><br> Intelligent Robots and Systems (IROS) 2022 <br> 
                       <a href="https://learning-dynamic-manipulation.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2204.02390">Paper</a>
                       &nbsp;&nbsp;&bull;&nbsp;&nbsp; <a href="https://github.com/jimmyyhwu/learning-dynamic-manipulation">Code</a>
                       
                    </p>
                </div>



                <div class="research-proj">
                    <a href="https://flingbot.cs.columbia.edu/" class="research-thumb"><img src="images/projects/flingbot-crop.gif" alt="" /></a>
                    <a href="https://flingbot.cs.columbia.edu/" class="research-proj-title">FlingBot: The Unreasonable Effectiveness of Dynamic Manipulations for Cloth Unfolding</a>
                    <p>
                       Huy Ha, Shuran Song <br>
					   Conference on Robot Learning (<b>CoRL2021</b>)<br>
					   <strong>Best System Paper Award</strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp; <!-- <strong>Oral Presentation</strong>&nbsp;&nbsp;&bull;&nbsp;&nbsp; -->
                       <a href="https://flingbot.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2105.03655">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/columbia-ai-robotics/flingbot">Code</a>
                       
                    </p>
                </div>


                
                <div class="research-proj">
                    <a href="https://fit2form.cs.columbia.edu/" class="research-thumb"><img src="images/projects/fit2form.gif" alt="" /></a>
                    <a href="https://fit2form.cs.columbia.edu/" class="research-proj-title">Fit2Form: 3D Generative Model for Robot Gripper Form Design</a>
                    <p>
                       Huy Ha*, Shubham Agrawal*, Shuran Song <br>
                       Conference on Robot Learning (CoRL) 2020 <br> 
                        <!-- <strong>Oral Presentation</strong>&nbsp;&nbsp;&bull;&nbsp;&nbsp; -->
                        <a href="https://fit2form.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/2011.06498">Paper</a>
                        <a href="https://github.com/columbia-ai-robotics/fit2form">Code</a>
                    </p>
                </div>

                

                <div class="research-proj">
                    <a href="http://graspinwild.cs.columbia.edu/" class="research-thumb"><img src="images/projects/graspinwild.jpg" alt="" /></a>
                    <a href="http://graspinwild.cs.columbia.edu/" class="research-proj-title">Grasping in the Wild: Learning 6DoF Closed-Loop Grasping from Low-Cost Demonstrations</a>
                    <p>
                        Shuran Song, Andy Zeng, Johnny Lee, Thomas Funkhouser<br>
                        Intelligent Robots and Systems (IROS) 2020 <br> Robotics and Automation Letters (RA-L) 2020</i><br>
                        <a href="http://graspinwild.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1912.04344">PDF</a>
                    </p>
                </div>

                <span id="morePubs">

                                <div class="research-proj">
                    <a href="https://jxu.ai/tandem/" class="research-thumb"><img src="images/projects/tandem.jpeg" alt="" /></a>
                    <a href="https://jxu.ai/tandem/" class="research-proj-title">TANDEM: Learning Joint Exploration and Decision Making with Tactile Sensors </a>
                    <p>
                       Jingxi Xu, Shuran Song, Matei Ciocarlie <br>
                       Robotics and Automation Letters (RA-L) 2022</i><br> Intelligent Robots and Systems (IROS) 2022 <br> 
                       <a href="https://jxu.ai/tandem/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2203.00798">Paper</a>
                       <!-- &nbsp;&nbsp;&bull;&nbsp;&nbsp; <a href="https://github.com/columbia-ai-robotics/">Code</a> -->
                       
                    </p>
                </div>

                 <div class="research-proj">
                    <a href="https://seat.cs.columbia.edu/" class="research-thumb"><img src="images/projects/seat.jpeg" alt="" /></a>
                    <a href="https://seat.cs.columbia.edu/" class="research-proj-title">Scene Editing as Teleoperation: A Case Study in 6DoF Kit Assembly</a>
                    <p>
                       Shubham Agrawal*, Yulong Li*, Jen-Shuo Liu, Steven K. Feiner, Shuran Song <br>
                       Intelligent Robots and Systems (IROS) 2022 <br> 
                       <a href="https://seat.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2110.04450">Paper</a>
                       <!-- &nbsp;&nbsp;&bull;&nbsp;&nbsp; <a href="https://github.com/columbia-ai-robotics/">Code</a> -->
                       
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://prior.allenai.org/projects/csr" class="research-thumb"><img src="images/projects/CSR.jpeg" alt="" /> </a>
                    <a href="https://prior.allenai.org/projects/csr" class="research-proj-title">Continuous Scene Representations for Embodied AI</a>
                    <p>
                       Samir Yitzhak Gadre, Kiana Ehsani, Shuran Song, Roozbeh Mottaghi, <br>
                       Proceedings of IEEE Conference on Computer Vision and Pattern Recognition <b> (CVPR 2022) <b> <br>
                       <a href="https://prior.allenai.org/projects/csr">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2203.17251">Paper</a>
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://ump-net.cs.columbia.edu/" class="research-thumb"><img src="images/projects/ump-net.jpeg" alt="" /></a>
                    <a href="https://ump-net.cs.columbia.edu/" class="research-proj-title">UMPNet: Universal Manipulation Policy Network for Articulated Objects</a>
                    <p>
                       Zhenjia Xu, Zhanpeng He, Shuran Song<br>
                       Robotics and Automation Letters (RA-L) and ICRA 2022 <br>
                       <a href="https://ump-net.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2109.05668">Paper</a>
                       <!-- &nbsp;&nbsp;&bull;&nbsp;&nbsp; <a href="https://github.com/columbia-ai-robotics/">Code</a> -->
                       
                    </p>
                </div>


                <div class="research-proj">
                    <a href="" class="research-thumb"><img src="images/projects/fish_sim.gif" alt="" /></a>
                    <a href="" class="research-proj-title"> FishGym: A High-Performance Physics-based Simulation Framework for Underwater Robot Learning </a>
                    <p>
                       Wenji Liu, Kai Bai, Xuming He, Shuran Song, Changxi Zheng, and Xiaopei Liu<br>
                       International Conference on Robotics and Automation <b> (ICRA 2022) </b> <br>
                       <a href="https://github.com/fish-gym/gym-fish">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="paper/fish-gym.pdf">Paper</a>
                       <!-- &nbsp;&nbsp;&bull;&nbsp;&nbsp; <a href="https://github.com/columbia-ai-robotics/">Code</a> -->
                       
                    </p>
                </div>


                

               

                <div class="research-proj">
                    <a href="" class="research-thumb"><img src="images/projects/se3pose.jpeg" alt="" /></a>
                    <a href="" class="research-proj-title">Leveraging SE(3) Equivariance for Self-supervised Category-Level Object Pose Estimation from Point Clouds </a>
                    <p>
                       Xiaolong Li, Yijia Weng, Li Yi, Leonidas Guibas, A. Lynn Abbott, Shuran Song, He Wang <br>
                       NeurIPS, 2021 <br>
                      
                       <a href="https://dragonlong.github.io/equi-pose/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2111.00190">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/dragonlong/equi-pose">Code</a>
                       
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://garmentnets.cs.columbia.edu/" class="research-thumb"><img src="images/projects/garmentnets.png" alt="" /></a>
                    <a href="https://garmentnets.cs.columbia.edu/" class="research-proj-title">GarmentNets: Category-Level Pose Estimation for Garments via Canonical Space Shape Completion </a>
                    <p>
                       Cheng Chi, Shuran Song <br>
                       IEEE International Conference on Computer Vision (<b>ICCV2021</b>)<br>
                         <a href="https://garmentnets.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/2104.05177">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://github.com/columbia-ai-robotics/garmentnets">Code</a>
                    </p>
                </div>


                <div class="research-proj">
                    <a href="https://atp.cs.columbia.edu/" class="research-thumb"><img src="images/projects/atp.png" alt="" /></a>
                    <a href="https://atp.cs.columbia.edu/" class="research-proj-title">Act the Part: Learning Interaction Strategies for Articulated Object Part Discovery </a>
                    <p>
                       Samir Yitzhak Gadre, Kiana Ehsani, Shuran Song <br>
                       IEEE International Conference on Computer Vision (<b>ICCV2021</b>)<br>
                         <a href="https://atp.cs.columbia.edu/">Webpage (with online Demo!)</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/2105.01047">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="">Code</a>
                    </p>
                </div>



                <div class="research-proj">
                    <a href="http://crlab.cs.columbia.edu/dynamic_grasping/" class="research-thumb"><img src="images/projects/dynmaic-grasping.png" alt="" /></a>
                    <a href="http://crlab.cs.columbia.edu/dynamic_grasping/" class="research-proj-title">Dynamic Grasping with Reachability and Motion Awareness</a>
                    <p>
                       Iretiayo Akinola*, Jingxi Xu*, Shuran Song, and Peter Allen <br>
                       International Conference on Intelligent Robots and Systems (IROS) 2021<br>
                         <a href="http://crlab.cs.columbia.edu/dynamic_grasping/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/2103.10562">Paper</a> 
                       
                    </p>
                </div>


                <div class="research-proj">
                    <a href="https://adagrasp.cs.columbia.edu/" class="research-thumb"><img src="images/projects/adagrasp2.gif" alt="" /></a>
                    <a href="https://adagrasp.cs.columbia.edu/" class="research-proj-title">AdaGrasp: Learning an Adaptive Gripper-Aware Grasping Policy</a>
                    <p>
                       Zhenjia Xu, Beichun Qi, Shubham Agrawal, Shuran Song <br>
                       International Conference on Robotics and Automation <b> (ICRA 2021) </b> <br>
                         <a href="https://adagrasp.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/2011.14206">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://github.com/columbia-ai-robotics/adagrasp">Code</a>
                    </p>
                </div>

                

                
                
                <div class="research-proj ">
                    <a href="https://spatial-intention-maps.cs.princeton.edu/" class="research-thumb"><img src="images/projects/sim.jpg" alt="" /></a>
                    <a href="https://spatial-intention-maps.cs.princeton.edu/" class="research-proj-title">Spatial Intention Maps for Multi-Agent Mobile Manipulation</a>
                    <p>
                       Jimmy Wu, Xingyuan Sun, Andy Zeng, Shuran Song, Szymon Rusinkiewicz, Thomas Funkhouser <br>
                       International Conference on Robotics and Automation <b> (ICRA 2021) </b> <br>
                         <a href="https://spatial-intention-maps.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/2011.14206">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://github.com/jimmyyhwu/spatial-intention-maps">Code</a>
                    </p>
                </div>

                 <div class="research-proj ">
                    <a href="http://www.cs.columbia.edu/~bchen/vpttob//" class="research-thumb"><img src="images/projects/VPT.jpg" alt="" /></a>
                    <a href="http://www.cs.columbia.edu/~bchen/vpttob/" class="research-proj-title">Visual Perspective Taking for Opponent Behavior Modeling</a>
                    <p>
                       Boyuan Chen, Yuhang Hu, Robert Kwiatkowski, Shuran Song, Hod Lipson <br>
                       International Conference on Robotics and Automation <b> (ICRA 2021) </b> <br>
                         <a href="http://www.cs.columbia.edu/~bchen/vpttob/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/2103.12710">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="">Code</a>
                    </p>
                </div>
                
                <div class="research-proj ">
                    <a href="https://sscnav.cs.columbia.edu/" class="research-thumb"><img src="images/projects/SSCNav.gif" alt="" /></a>
                    <a href="https://sscnav.cs.columbia.edu/" class="research-proj-title">SSCNav: Confidence-Aware Semantic Scene Completion for Visual Semantic Navigation</a>
                    <p>
                       Yiqing Liang, Boyuan Chen, Shuran Song <br>
                       International Conference on Robotics and Automation <b> (ICRA 2021) </b> <br>
                         <a href="https://sscnav.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/2012.04512">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://github.com/columbia-ai-robotics/SSCNav">Code</a>
                    </p>
                </div>
                
                <div class="research-proj ">
                    <a href="https://dsr-net.cs.columbia.edu/" class="research-thumb"><img src="images/projects/spotlight_dsrnet.jpg" alt="" /></a>
                    <a href="https://dsr-net.cs.columbia.edu/" class="research-proj-title">Learning 3D Dynamic Scene Representations for Robot Manipulation</a>
                    <p>
                       Zhenjia Xu*, Zhanpeng He*, Jiajun Wu, Shuran Song <br>
                       Conference on Robot Learning (CoRL) 2020 <br> 
                        <!-- <strong>Oral Presentation</strong>&nbsp;&nbsp;&bull;&nbsp;&nbsp; -->
                        <a href="https://dsr-net.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/2011.01968">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://github.com/columbia-ai-robotics/dsr">Code</a>
                    </p>
                </div>

                <div class="research-proj ">
                    <a href="https://multiarm.cs.columbia.edu/" class="research-thumb"><img src="images/projects/multiarm.gif" alt="" /></a>
                    <a href="https://multiarm.cs.columbia.edu/" class="research-proj-title">Learning a Decentralized Multi-arm Motion Planner</a>
                    <p>
                       Huy Ha, Jingxi Xu, Shuran Song <br>
                       Conference on Robot Learning (CoRL) 2020 <br> 
                        <!-- <strong>Oral Presentation</strong>&nbsp;&nbsp;&bull;&nbsp;&nbsp; -->
                        <a href="https://multiarm.cs.columbia.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/2011.02608">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://github.com/columbia-ai-robotics/decentralized-multiarm">Code</a>
                    </p>
                </div>


                <div class="research-proj ">
                    <a href="" class="research-thumb"><img src="images/projects/ads-eccv2020.jpg" alt="" /></a>
                    <a href="" class="research-proj-title">Multi-task Learning Increases Adversarial Robustness </a>
                    <p>
                        Chengzhi Mao, Amogh Gupta, Vikram Nitin, Baishakhi Ray, Shuran Song, Junfeng Yang, Carl Vondrick<br>
                        European Conference on Computer Vision (ECCV) 2020 <br> 
                        <strong>Oral Presentation</strong>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <!-- <a href="">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp; -->
                        <a href="https://arxiv.org/abs/2007.07236v1">PDF</a>
                    </p>
                </div>

                <div class="research-proj ">
                    <a href="https://spatial-action-maps.cs.princeton.edu/" class="research-thumb"><img src="images/projects/rss20.png" alt="" /></a>
                    <a href="https://spatial-action-maps.cs.princeton.edu/" class="research-proj-title">Spatial Action Maps for Mobile Manipulation</a>
                    <p>
                        Jimmy Wu, Xingyuan Sun, Andy Zeng, Shuran Song, Johnny Lee, Szymon Rusinkiewicz, Thomas Funkhouser<br>
                        Robotics: Science and Systems (RSS) 2020 <br>
                        <a href="https://spatial-action-maps.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/2004.09141.pdf">PDF</a>
                    </p>
                </div>
                
                <div class="research-proj ">
                    <a href="https://articulated-pose.github.io/" class="research-thumb"><img src="images/projects/artpose.jpg" alt="" /></a>
                    <a href="https://articulated-pose.github.io/" class="research-proj-title">Category-Level Articulated Object Pose Estimation</a>
                    <p>
                        Xiaolong Li, He Wang, Li Yi, Leonidas Guibas, A. Lynn Abbott, Shuran Song<br>
                        <i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2020</i><br>
                        <strong>Oral Presentation</strong>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://articulated-pose.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1912.11913">PDF</a>
                    </p>
                </div>
               
                <div class="research-proj">
                    <a href="http://form2fit.github.io/" class="research-thumb"><img src="images/projects/form2fit.png" alt="" /></a>
                    <a href="http://form2fit.github.io/" class="research-proj-title">Form2Fit: Learning Shape Priors for Generalizable Assembly from Disassembly</a>
                    <p>
                        Kevin Zakka, Andy Zeng, Johnny Lee, Shuran Song<br>
                        International Conference on Robotics and Automation <b> (ICRA 2020) </b> <br>
                        <strong>Best Paper Award in Automation  Finalist</strong>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="http://form2fit.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1910.13675">PDF</a>
                    </p>
                </div>
                <div class="research-proj">
                    <a href="https://sites.google.com/view/cleargrasp" class="research-thumb"><img src="images/projects/cleargrasp.gif" alt="" /></a>
                    <a href="https://sites.google.com/view/cleargrasp" class="research-proj-title">ClearGrasp: 3D Shape Estimation of Transparent Objects for Manipulation</a>
                    <p>
                        Shreeyak S. Sajjan, Matthew Moore, Mike Pan, Ganesh Nagaraja, Johnny Lee, Andy Zeng, Shuran Song <br>
                        International Conference on Robotics and Automation <b> (ICRA 2020) </b><br>
                        <a href="https://sites.google.com/view/cleargrasp">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1910.02550">PDF</a>
                    </p>
                </div>

                <div class="research-proj ">
                    <a href="https://yenchenlin.me/vision2action/" class="research-thumb"><img src="images/projects/vision2action.png" alt="" /></a>
                    <a href="https://yenchenlin.me/vision2action/" class="research-proj-title"> Learning to See before Learning to Act: Visual Pre-training for Manipulation</a>
                    <p>
                        Lin Yen-Chen, Andy Zeng, Shuran Song, Phillip Isola, Tsung-Yi Lin <br>
                        International Conference on Robotics and Automation <b> (ICRA 2020) </b> <br>
                        <a href="https://yenchenlin.me/vision2action/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://yenchenlin.me/vision2action/">PDF</a>
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://tossingbot.cs.princeton.edu/" class="research-thumb"><img src="images/projects/tossing.jpg" alt="" /></a>
                    <a href="https://tossingbot.cs.princeton.edu/" class="research-proj-title"> TossingBot: Learning to Throw Arbitrary Objects with Residual Physics </a>
                    <p>
                        Andy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Alberto Rodriguez, Thomas Funkhouser <br>   
                        Robotics: Science and Systems 2019  <b>(RSS 2019)</b> <br> IEEE Transactions on Robotics   <b>(T-RO 2020)</b> <br>
                        <strong>Best System Paper Award</strong> &nbsp;&nbsp; <strong>Best Student Paper Finalist</strong>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://tossingbot.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://tossingbot.cs.princeton.edu/">PDF</a>
                    </p>
                </div>

                <div class="research-proj">
                    <a href="http://www.zhenjiaxu.com/DensePhysNet/" class="research-thumb"><img src="images/projects/densephysnet-crop.gif" alt="" /></a>
                    <a href="http://www.zhenjiaxu.com/DensePhysNet/" class="research-proj-title"> DensePhysNet: Learning Dense Physical Object Representations via Multi-step Dynamic Interactions</a>
                    <p>
                        Zhenjia Xu, Jiajun Wu, Andy Zeng, Joshua Tenenbaum, Shuran Song <br>
                        Robotics: Science and Systems 2019 <b>(RSS 2019)</b> <br>  
                        <!-- <strong>Best System Paper Award</strong> &nbsp;&nbsp; <strong>Best Student Paper Finalist</strong>&nbsp;&nbsp;&bull;&nbsp;&nbsp; -->
                        <a href="http://www.zhenjiaxu.com/DensePhysNet/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1906.03853.pdf">PDF</a>
                    </p>
                </div>


                

                <div class="research-proj ">
                    <a href="https://geometry.stanford.edu/projects/NOCS_CVPR2019/" class="research-thumb"><img src="images/projects/PoseRCNN2019.jpg" alt="" /></a>
                    <a href="https://geometry.stanford.edu/projects/NOCS_CVPR2019/" class="research-proj-title">Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation</a>
                    <p>
                        He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, <b>Shuran Song </b>, Leonidas J. Guibasi<br>
                        Proceedings of 32th IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR2019</b>)<br> 
                        <strong>Oral Presentation</strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp;  
                        <a href="https://geometry.stanford.edu/projects/NOCS_CVPR2019/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1906.07370.pdf">PDF</a>
                    </p>
                </div>

                <div class="research-proj ">
                    <a href="https://illumination.cs.princeton.edu/" class="research-thumb"><img src="images/projects/illumination.png" alt="" /></a>
                    <a href="https://illumination.cs.princeton.edu/" class="research-proj-title">Neural Illumination: Lighting Prediction for Indoor Environments</a>
                    <p>
                        <b>Shuran Song</b> and Thomas Funkhouser<br>
                        Proceedings of 32th IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR2019</b>)<br>
                        <strong>Oral Presentation</strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp;  
                        <a href="https://illumination.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1906.07370.pdf">PDF</a>
                    </p>
                </div>

                <div class="research-proj ">
                    <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Michelle_Guo_Neural_Graph_Matching_ECCV_2018_paper.pdf" class="research-thumb"><img src="images/projects/graph_learning.jpg" alt="" /></a>
                    <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Michelle_Guo_Neural_Graph_Matching_ECCV_2018_paper.pdf" class="research-proj-title">Neural Graph Matching Networks for Fewshot 3D Action Recognition</a>
                    <p>
                        Michelle Guo, Edward Chou, <b>Shuran Song</b>, De-An Huang, Serena Yeung, Li Fei-Fei<br>
                        European Conference on Computer Vision (<b>ECCV2018</b>)<br>
                        <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Michelle_Guo_Neural_Graph_Matching_ECCV_2018_paper.pdf">PDF</a>
                    </p>
                </div>

                <div class="research-proj nonRep">
                    <a href="http://vpg.cs.princeton.edu/" class="research-thumb"><img src="images/projects/vpg.jpg" alt="" /></a>
                    <a href="http://vpg.cs.princeton.edu/" class="research-proj-title">Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning</a>
                    <p>
                        A. Zeng, <b>S. Song</b>, S. Welker, J. Lee, A. Rodriguez, T. Funkhouser <br>
                        Intelligent Robots and Systems (IROS) 2020 <br> 
                        <strong> Best Cognitive Robotics Paper Award Finalist</strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp; 

                        <a href="https://vpg.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1803.09956.pdf">PDF</a>
                    </p>
                </div>

                <div class="research-proj ">
                    <a href="http://im2pano3d.cs.princeton.edu/" class="research-thumb"><img src="images/projects/im2pano3d.jpg" alt="" /></a>
                    <a href="http://im2pano3d.cs.princeton.edu/" class="research-proj-title">Im2Pano3D: Extrapolating 360Â° Structure and Semantics Beyond the Field of View</a>
                    <p>
                        <b>S. Song</b>, A. Zeng, A. X. Chang, M. Savva, S. Savarese, T. Funkhouser <br>
                        Proceedings of 31th IEEE Conference on Computer Vision and Pattern Recognition <b>CVPR2018</b><br> 
                        <strong> Oral Presentation </strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp;  
                        <a href="https://im2pano3d.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="http://arxiv.org/abs/1712.04569">PDF</a>
                    </p>
                </div>


                <div class="research-proj">
                    <a href="http://arc.cs.princeton.edu/" class="research-thumb"><img src="images/projects/arc.jpg" alt="" /></a>
                    <a href="http://arc.cs.princeton.edu/" class="research-proj-title">Robotic Pick-and-Place of Novel Objects in Clutter with Multi-Affordance Grasping and Cross-Domain Image Matching</a>
                    <p>
                       A. Zeng, <b>S. Song</b>, K. Yu, E. Donlon, F. R. Hogan, M. Bauza, D. Ma, O. Taylor, M. Liu, E. Romo, N. Fazeli, F. Alet, N. C. Dafle, R. Holladay, I. Morona, P. Q. Nair, D. Green, I. Taylor, W. Liu, T. Funkhouser, A. Rodriguez   <b>(ICRA2018)</b><br>
                       
                        <strong> Amazon Robotics Best Systems Paper Award </strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp;  
                        <a href="https://arc.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1710.01330.pdf">PDF</a>
                    </p>
                </div>


                 <div class="research-proj ">
                    <a href="http://sscnet.cs.princeton.edu/" class="research-thumb"><img src="images/projects/ssc.jpg" alt="" /></a>
                    <a href="http://sscnet.cs.princeton.edu/" class="research-proj-title">Semantic Scene Completion from a Single Depth Image</a>
                    <p>
                        <b>S. Song</b>, F. Yu, A. Zeng, A. Chang, M. Savva, T. Funkhouser <br>
                        Proceedings of 30th IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR2017</b>)<br>
                        <strong> Oral Presentation </strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp;  
                        <a href="https://sscnet.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1611.08974.pdf">PDF</a>
                    </p>
                </div>

                <div class="research-proj ">
                    <a href="http://3dmatch.cs.princeton.edu/" class="research-thumb"><img src="images/projects/3dmatch.jpg" alt="" /></a>
                    <a href="http://3dmatch.cs.princeton.edu/" class="research-proj-title">3DMatch: Learning the Matching of Local 3D Geometry in Range Scans</a>
                    <p>
                        A. Zeng, <b>S. Song</b>, M. NieÃŸner, M. Fisher, J. Xiao and T. Funkhouser.<br>
                        Proceedings of 30th IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR2017</b>)<br>
                        <strong> Oral Presentation </strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp;  
                        <a href="https://3dmatch.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="http://3dvision.princeton.edu/projects/2016/3DMatch/paper_v2.pdf">PDF</a>
                    </p>
                    <br>
                </div>

                

                <div class="research-proj ">
                    <a href="http://pbr.cs.princeton.edu/" class="research-thumb"><img src="images/projects/pbr.jpg" alt="" /></a>
                    <a href="http://pbr.cs.princeton.edu/" class="research-proj-title">Physically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks</a>
                    <p>
                        Y. Zhang<sup>*</sup>, <b>S. Song<sup>*</sup></b>,  E. Yumer, M. Savva, J. Lee, H. Jin, T. Funkhouser.<br>
                        Proceedings of 30th IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR2017</b>)<br>
                        <strong> Oral Presentation </strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp;  
                        <a href="https://arxiv.org/abs/1612.07429">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1612.07429">PDF</a>
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://niessner.github.io/Matterport/" class="research-thumb"><img src="images/projects/matterport.jpg" alt="" /></a>
                    <a href="https://niessner.github.io/Matterport/" class="research-proj-title">Matterport3D: Learning from RGB-D Data in Indoor Environments</a>
                    <p>
                        A. X. Chang, A. Dai, T. Funkhouser, M. Halber, M. NieÃŸner, M. Savva, <b> S. Song </b>, A. Zeng, Y. Zhang<br>
                        IEEE International Conference on 3D Vision  (<b>3DV 2017</b>)<br>
                        <!-- <strong> Oral Presentation </strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp;   -->
                        <a href="https://niessner.github.io/Matterport/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1709.06158.pdf">PDF</a>

                    </p>
                    <br>
                </div>


                <div class="research-proj">
                    <a href="http://apc.cs.princeton.edu" class="research-thumb"><img src="images/projects/robot.png" alt="" /></a>
                    <a href="http://apc.cs.princeton.edu" class="research-proj-title">Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge</a>
                    <p>
                        A. Zeng, K.T. Yu, <b>S. Song</b>, D. Suo, E. Walker Jr., A. Rodriguez, and J. Xiao<br>
                        International Conference on Robotics and Automation (<b>ICRA2017</b>) <br>
                       <!--  <strong> Oral Presentation </strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp;   -->
                        <a href="http://apc.cs.princeton.edu">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1609.09475v1.pdf">PDF</a>
                    </p>
                </div>

                <div class="research-proj">
                    <a href="http://apc.cs.princeton.edu" class="research-thumb"><img src="images/projects/thumbnail(4).jpg" alt="" /></a>
                    <a href="http://apc.cs.princeton.edu" class="research-proj-title">Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images</a>
                    <p>
                        <b>S. Song</b>, and J. Xiao.<br>
                        Proceedings of 29th IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR2016</b>)<br>
                       <!--  <strong> Oral Presentation </strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp;   -->
                        <a href="http://dss.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="http://3dvision.princeton.edu/projects/2015/DSS/paper.pdf">PDF</a>
                    </p>
                    <br>
                </div>


                <div class="research-proj">
                    <a href="http://shapenet.org/" class="research-thumb"><img src="images/projects/shapenet.jpg" alt="" /></a>
                    <a href="http://shapenet.org/" class="research-proj-title">ShapeNet: An Information-Rich 3D Model Repository</a>
                    <p>
                        A. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, <b>S. Song</b>, H. Su, J. Xiao, L. Yi, and F. Yu.<br>
                        arXiv:1512.03012 [cs.CV] 9 Dec 2015<br>
                       <!--  <strong> Oral Presentation </strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp;   -->
                        <a href="http://shapenet.org/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="http://3dvision.princeton.edu/projects/2015/ShapeNet/paper.pdf">PDF</a>
                    </p>
                </div>



                <div class="research-proj">
                    <a href="http://rgbd.cs.princeton.edu/" class="research-thumb"><img src="images/projects/sunrgbd.jpg" alt="" /></a>
                    <a href="http://rgbd.cs.princeton.edu/" class="research-proj-title">SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite</a>
                    <p>
                        <b>S. Song</b>, S. Lichtenberg and J. Xiao<br>
                        Proceedings of 28th IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR2015</b>)<br>
                        <a style="color: #005C91;" href="http://techtalks.tv/talks/sun-rgb-d-a-rgb-d-scene-understanding-benchmark-suite/61578/">  <strong> Oral Presentation [Watch it on Techtalks] </strong> </a> &nbsp;&nbsp;&bull;&nbsp;&nbsp;  
                        <a href="http://rgbd.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="http://3dvision.princeton.edu/projects/2015/SUNrgbd/paper.pdf">PDF</a>
                    </p>
                </div>


                <div class="research-proj">
                    <a href="http://3dshapenets.cs.princeton.edu/" class="research-thumb"><img src="images/projects/3dshapenet.jpg" alt="" /></a>
                    <a href="http://3dshapenets.cs.princeton.edu/" class="research-proj-title">SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite</a>
                    <p>
                        Z. Wu, <b>S. Song</b>, A. Khosla, F. Yu, L. Zhang, X. Tang and J. Xiao<br>
                        Proceedings of 28th IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR2015</b>)<br>
                        <a style="color: #005C91;" href="http://techtalks.tv/talks/3d-shapenets-a-deep-representation-for-volumetric-shapes/61589/">  <strong> Oral Presentation [Watch it on Techtalks] </strong> </a> &nbsp;&nbsp;&bull;&nbsp;&nbsp;  
                        <a href="http://3dshapenets.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="http://3dvision.princeton.edu/projects/2014/3DShapeNets/paper.pdf">PDF</a>
                    </p>
                </div>

                <div class="research-proj ">
                    <a href="http://vision.princeton.edu/projects/2015/RobotInARoom/" class="research-thumb"><img src="images/projects/thumbnail(8).jpg" alt="" /></a>
                    <a href="http://vision.princeton.edu/projects/2015/RobotInARoom/" class="research-proj-title">Robot In a Room: Toward Perfect Object Recognition in Closed Environments</a>
                    <p>
                        <b>S. Song</b>, L. Zhang, and J. Xiao.<br>
                        arXiv:1507.02703 [cs.CV] 9 Jul 2015<br>
                        <a href="http://vision.princeton.edu/projects/2015/RobotInARoom/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="http://3dvision.princeton.edu/projects/2015/RobotInARoom/paper.pdf">PDF</a>
                    </p>
                </div>


                <div class="research-proj ">
                    <a href="http://lsun.yf.io/" class="research-thumb"><img src="images/projects/lsun.jpg" alt="" /></a>
                    <a href="http://lsun.yf.io/" class="research-proj-title">Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop</a>
                    <p>
                        F. Yu,  A. Seff,  Y. Zhang, <b>S. Song</b> and J. Xiao.<br>
                        arXiv:1506.03365 [cs.CV] 10 Jun 2015<br>
                        <a href="http://lsun.yf.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="http://arxiv.org/abs/1506.03365">PDF</a>
                    </p>
                </div>
                
                <div class="research-proj">
                    <a href="http://slidingshapes.cs.princeton.edu/" class="research-thumb"><img src="images/projects/thumbnail(10).jpg" alt="" /></a>
                    <a href="http://slidingshapes.cs.princeton.edu/" class="research-proj-title">Sliding Shapes for 3D Object Detection in Depth Images</a>
                    <p>
                        <b>S. Song</b> and J. Xiao<br>
                        Proceedings of the 13th European Conference on Computer Vision (<b>ECCV2014</b>)<br>
                        <a style="color: #005C91;" href="http://videolectures.net/eccv2014_song_depth_images/">  <strong> Oral Presentation [Watch it on Videolectures] </strong> </a> &nbsp;&nbsp;&bull;&nbsp;&nbsp;  
                        <a href="http://slidingshapes.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="http://3dvision.princeton.edu/projects/2014/SlidingShapes/paper.pdf">PDF</a>
                    </p>
                    <br>
                </div>


                <div class="research-proj">
                    <a href="http://slidingshapes.cs.princeton.edu/" class="research-thumb"><img src="images/projects/thumbnail(11).jpg" alt="" /></a>
                    <a href="http://slidingshapes.cs.princeton.edu/" class="research-proj-title">PanoContext: A Whole-room 3D Context Model for Panoramic Scene Understanding</a>
                    <p>
                        Y. Zhang, <b>S. Song</b>, P. Tan, and J. Xiao<br>
                        Proceedings of the 13th European Conference on Computer Vision (<b>ECCV2014</b>)<br>
                        <a style="color: #005C91;" href="http://videolectures.net/eccv2014_song_depth_images/">  <strong> Oral Presentation [Watch it on Videolectures] </strong> </a> &nbsp;&nbsp;&bull;&nbsp;&nbsp;  
                        <a href="http://panocontext.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="http://3dvision.princeton.edu/projects/2014/PanoContext/paper.pdf">PDF</a>
                    </p>
                    <br>
                </div>


                <div class="research-proj">
                    <a href="http://tracking.cs.princeton.edu/" class="research-thumb"><img src="images/projects/tracking.jpg" alt="" /></a>
                    <a href="http://tracking.cs.princeton.edu/" class="research-proj-title">Tracking Revisited using RGBD Camera: Unified Benchmark and Baselines</a>
                    <p>
                         <b>S. Song</b> and J. Xiao<br>
                         Proceedings of 14th IEEE International Conference on Computer Vision (<b>ICCV2013</b>)<br>
                        <a href="http://tracking.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="http://3dvision.princeton.edu/projects/2013/tracking/paper.pdf">PDF</a>
                    </p>
                </div>


                </span>

                <div onclick="togglePubs()" id="morePubsBtn" class="showBtn"><a>Show more...</a></div>
                <div onclick="togglePubs()" id="lessPubsBtn" class="showBtn"><a>Show less...</a></div>
                <div style="clear: both;"></div>
            </div>

            <div class="divider"></div>
            <div class="section tesching">
            <h1>Teaching</h1>
                <p>
                    <ul>
                        <li><a href="https://sites.google.com/view/win23-24-ee227/home">EE/CS227: Robot Perception</a> </li>
                        <li><a href="https://sites.google.com/view/fall23-ee381/home">EE/CS 381: Sensorimotor Learning for Embodied Agents </a> </li>
                        <li><a href="https://sites.google.com/view/spring2022-coms4733/home">COMS 4733 Computational Aspects of Robotics, 2020-2023</a> </li>
                        <li><a href="https://sites.google.com/view/robot-learning-2019fall/home">COMS 6998 Topics in Robot Learning, 2020-2023</a> </li>
                        <!-- <li>Teaching Assistant: <a href="http://www.cs.princeton.edu/courses/archive/spr15/cos126/syllabus.html">Princeton COS126 General Computer Science</a>, 2015</li>
                        <li>Teaching Assistant: <a href="https://sites.google.com/view/sqml/teaching-experience/elec1100-introduction-to-electro-robot-design"> HKUST ELEC1100 Introduction to Electro-Robot Design </a>, 2011,2012</li>
                        <li>Teaching Assistant: HKUST ELEC1200A System View of Communications: from Signals to Packets, 2011</li> -->
                    </ul>
                </p>
            </div>

            <div class="divider"></div>
            <!-- <div class="section sponsors">
                <h1>Sponsors</h1>
                <div class="sponsor-thumb">
                    <a href="https://www.tri.global/"><img src="images/sponsors/toyota-research-institute.jpg" alt="" /></a>
                    <a href="https://www.amazonrobotics.com/"><img src="images/sponsors/amazon-robotics.jpg" alt="" /></a>
                    <a href="https://ai.google/"><img src="images/sponsors/google-ai.png" alt="" /></a>
                    <a href=""><img src="images/sponsors/NSF_4-Color_bitmap_Logo.png" alt="" /></a>
                </div>
                <div style="clear: both;"></div> -->
            </div>
        </div>

        <script>
            function toggleNews() {
              var moreNews = document.getElementById("moreNews");
              var moreNewsBtn = document.getElementById("moreNewsBtn");
              var lessNewsBtn = document.getElementById("lessNewsBtn");
              if (moreNewsBtn.style.display === "none") {
                moreNews.style.display = "none";
                moreNewsBtn.style.display = "inline";
                lessNewsBtn.style.display = "none";
              } else {
                moreNews.style.display = "inline";
                moreNewsBtn.style.display = "none";
                lessNewsBtn.style.display = "inline";
              }
            }


            function toggleTalks() {
              var moreTalks = document.getElementById("moreTalks");
              var moreTalksBtn = document.getElementById("moreTalksBtn");
              var lessTalksBtn = document.getElementById("lessTalksBtn");
              
              if (moreTalksBtn.style.display === "none") {
                moreTalks.style.display = "none";
                moreTalksBtn.style.display = "inline";
                lessTalksBtn.style.display = "none";
              } else {
                moreTalks.style.display = "inline";
                moreTalksBtn.style.display = "none";
                lessTalksBtn.style.display = "inline";
              }
            }
            function togglePubs() {
              var morePubs = document.getElementById("morePubs");
              var morePubsBtn = document.getElementById("morePubsBtn");
              var lessPubsBtn = document.getElementById("lessPubsBtn");
              if (morePubsBtn.style.display === "none") {
                morePubs.style.display = "none";
                morePubsBtn.style.display = "inline";
                lessPubsBtn.style.display = "none";
              } else {
                morePubs.style.display = "inline";
                morePubsBtn.style.display = "none";
                lessPubsBtn.style.display = "inline";
              }
            }

            function showRep() {
              var repBtn = document.getElementById("repBtn");
              var showAllBtn = document.getElementById("showAllBtn");
              repBtn.style.color = "#49bf9d";
              repBtn.style.borderBottom = "1px solid #a4dfce";
              showAllBtn.style.color = "#191e3f";
              showAllBtn.style.borderBottom = "none";
              var nonRep = document.getElementsByClassName("nonRep");
              for (var i = 0; i < nonRep.length; i++) {
                nonRep.item(i).style.display = "none";
              }
            }

            function hideRep() {
              var repBtn = document.getElementById("repBtn");
              var showAllBtn = document.getElementById("showAllBtn");
              repBtn.style.color = "#191e3f";
              repBtn.style.borderBottom = "none";
              showAllBtn.style.color = "#49bf9d";
              showAllBtn.style.borderBottom = "1px solid #a4dfce";
              var nonRep = document.getElementsByClassName("nonRep");
              for (var i = 0; i < nonRep.length; i++) {
                nonRep.item(i).style.display = "table";
              }
            }
        </script>
    </body>
</html>